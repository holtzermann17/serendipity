\bigskip

\noindent In our thought experiment, we focus on the case of
hypothetical discussions and exchange of views between computational
poetry systems as our example of a situation where social
circumstances could encourage serendipity. We note that similar ideas
would apply for prose and, with further adaptation, other arts.

\paragraph{Thought Experiment: Prepared mind.}
Participating systems need to be able to follow the protocol.  This
means that participating systems will need components like those
listed above. The {\tt listening} and {\tt questions} components of
the protocol correspond to $p$ and $p^{\prime}$ our model of
serendipity.  The corresponding ``comment generator'' and ``feedback
integrator'' modules in the architecture represent the primary points
of interface to the outside world.  In principle these modules need to
be prepared to deal (more or less thoughtfully) with \emph{any} text,
and in turn, with \emph{any} comment on that text.  Certain limits may
be agreed in advance; e.g.~as to genre or length in the case of texts,
and what constitutes an acceptable comment.  The ``feedback
explainer'' is closely connected with the ``comment generator'' and in
an implementation of this model they would presumably share a
codebase.  The loop for learning by asking questions as they arise is
reminiscent of the operating strategy of {\sf SHRDLU}
\cite{winograd1972understanding}.

Importantly, one of the most relevant preparations would be prior
participation in Workshop dialogues.  A system with prior experience
in the Workshop may have a catalogue of outstanding unresolved, or
partially resolved, problems (denoted ``X'' in the schematic above).
Embodied in code, they may drive comments, questions, and other
behaviour -- and they may be answered in unexpected ways.

\paragraph{Thought Experiment: Serendipity triggers.}

Although the poem is under the control of the initial generative
subsystem, it is \emph{not} under control of the listening subsystem.
The listening subsystem expects some poem, but it does not know what
poem to expect.  In this sense, the poem constitutes a serendipity
trigger $T$, not only for the listening subsystem, but for the
Workshop system as a whole.
%
To expand this point, note that there may be several listeners, each
sharing their own feedback and listening to the feedback presented by
others (which, again, is outside of their direct control).  This
creates further potential for serendipity, since each listener can
learn what others see in the poem.  More formally, in this case
$T^\star$ may seen as an evolving vector with shared state, but viewed
and handled from different perspectives.  With multiple agents
involved in the discussion, the ``comment generator'' module would
expand to contain its own feedback loops.

\paragraph{Thought Experiment: Bridge.}

Feedback on portions of the poem may lead the system to identify new
problems, indeed, new \emph{types} of problems that it hadn't
considered before.  The most immediately feasible case is one in which
the critic is a programmer who can directly program new concepts into
the computer \cite<cf.>{winograd1972understanding}.  However, it would
be hard to call that ``serendipity.''

We can also ask whether agents can build new concepts \emph{without}
outside intervention, starting with some basic concepts and abilities
related to poetry (e.g.~definitions of words, valence of sentiments,
metre, repetition, density, etc.) and code (e.g.~the data, functions,
and macros in which the poetic concepts and workshop protocols are
embodied).  Some notable early experiments with concept invention have been
fraught with questions about autonomy
\cite{ritchie1984case,lenat1984and}. \citeA{colton2002automated}
presented a system that was convincingly autonomous -- the system was
able to generate interesting novel conjectures that surprised its author.
However, \citeA{pease2013discussion} note that this system was not
convincingly serendipitous: ``we had to willingly make the system less
effective to encourage incidents which onto which we might project the
word serendipity.''

One cognitively inspired hypothesis that could describe the
serendipitous formation of new concepts is the notion that the
development of new concepts is closely related to formation of new
sensory experiences \cite{milan2013kiki}.  If the workshop
participants have the capacity to identify the distinctive features of
a given poem, then training via a machine learning or genetic
algorithm approach could be used assemble a battery of existing
low-level tools that can approximate the effect.  Relatedly, a
compression process could seek to produce a given complex poetic
effect with a maximally-succinct
algorithm \cite<cf.>{schmidhuber2007simple}.
%
The key point is that feedback on the poem -- simply describing what
is in the poem from several different points of view -- can be used to
define new problems for the system to solve.  This is not simply a
matter of decomposing the poem into pieces, but also of reconstructing
the way in which the pieces work together.  This is one of the
functions of the {\tt questions} step corresponding to $p^{\prime}$ in
our formalism: they offer the poet the opportunity to enquire about
how different pieces of feedback fit together, and learn more about
where they come from.  Although computers are currently nowhere close,
the reconstructive process may steadily approach the ideal case --
familiar to humans -- of relating to the sentiment expressed by the
poem as a whole \cite[p. 209]{bergson1983creative}.

%% Several of us are involved with a contemporary project
%% \cite{coinvent14} to develop a formal theory of concept invention,
%% focusing on \emph{concept blending}.  The additive or subtractive
%% blending of existing poetry profiles may be another way to create new
%% concepts.


%% should be possible Modifer Grammar
%% Counting Breathing Position Distribution Phonics Rhythm Repetition
%% Thematic Narrative Entropy

\paragraph{Thought Experiment: Result.}

The final step is to take the problem or problems that were
identified, and write new code to solve them.  Several strategies for
generating a result $R$, in the form of new code, were described
above.  Now the system evaluates the new code to see whether it holds
promise.  In order to do this, it must have a way to carry out an
evaluation and judge whether $|R|>0$.  In the most straightforward
case, it would simply make changes to the draft poem that seem to
improve it in some way.  For example, the poet might remove or alter material that
elicited a negative response from a critic.  The system may proceed to
update its modules related to poetry generation.  Notably, it may also update its own
feedback modules, after reflecting on questions like: ``How might the
critic have detected that feature in my poem?''

\paragraph{Thought Experiment: Likelihood scores and potential value.}
Given most statements in natural language are new, we can assume that
most poems consumed by the system would never have been seen before,
and the chance of observing a given serendipity trigger would be very
small.  While the likelihood of detecting an \emph{already-known}
poetic feature (e.g.~rhyming, alliteration) in a given poem where it
is present is presumably fairly high, we are particularly interested
in the cases of where a \emph{novel} poetic feature can be discovered (e.g.~understanding Yeats's symbolic use of hawks\footnote{\citeA{perez1995yeats}.}), and this
would remain an infrequent occurrence, at least until the system as a
whole became very good at reading poems.  Indeed, a sophisticated
system would already have an extensive catalogue of poetic features,
and the chance of detecting a new and valuable feature might be low
for other reasons.  The chance that a newly-observed feature will
result in usable code seems relatively high, although this remains to
be shown in practice.  Presumably many novel poetic ideas would be seen as
high-value at first, while only some of these ideas will prove to have
lasting value.  Our likelihood score would be
$\mathit{low}\times\mathit{low}\times\mathit{high}$, or low overall,
and value would be varied, with at least some high-valued cases
deserving the description ``highly serendipitous.''

\paragraph{Thought Experiment: Environmental factors.}
The system would set up its own internal dynamics, but it could also
provide an interface for human poets to share their poetry and their
critical remarks.  There is one primary context, the Workshop, shared
by all participants.  The primary tasks envisaged in the system design
are \emph{poetry generation} and \emph{code generation}.  Although
these are different tasks, they may have similar features (i.e.~both may present opportunities to learn from feedback).
Influences could be highly multiple, including many very different kinds of poetry
and various approaches from NLP.
