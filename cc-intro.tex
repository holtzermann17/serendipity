\section{Serendipity in computational systems} \label{sec:computational-serendipity}

The 13 criteria from Section \ref{sec:literature-review} specify the
conditions and preconditions that are conducive to serendipitous
discovery.  These criteria have been further formalised
in Section \ref{specs-overview}.
% 
\citeA{pease2013discussion} used a slightly different version of the
SPECS criteria to discuss three examples of serendipitous behaviour:
in dynamic investigation problems, model generation, and poetry
flowcharts.  Two additional examples using the revised criteria are
described below.  These example serve the purpose of illustrating our
revised criteria, and also show forays of computational intelligence
into domains known for serendipity in their everyday cultural context.
We then turn to a more elaborated thought experiment that evaluates
these ideas in the course of developing a new system design.

% \input{writers-workshop-background-long}

\subsection{Case Studies: Prior art}

\paragraph{Evolutionary music improvisation systems.}

\citeA{jordanous10} reported a computational jazz improvisation system using genetic algorithms. Genetic algorithms, and evolutionary computing more generally, could encourage computational serendipity. We examine Jordanous's system (later given the name {\em GAmprovising} \cite{jordanous:12}) in the context of our model of computational serendipity: to what extent does GAmprovising model serendipity?


% \paragraph{{[}To add: HR.{]}}

\paragraph{Recommender systems.} 

As discussed in Section \ref{sec:related}, recommender systems are one
of the primary contexts in computing where serendipity is seen to play
a role.  As we noted, these systems mostly focus on discovery.
Nevertheless, certain architectures that also take account of
invention would match all of criteria described by our model.  Here we
draw on the observation that recommender systems not \emph{stimulate}
serendipitous discovery for the user: they also have the task of
\emph{simulating} when this is likely to occur.

A recommendation is typically provided if the system suspects that the
item will be likely to introduce ideas that are close to what the user
knows, but that will be unexpected.  In other words, the system aims
to stimulate serendipity for the user. For example, a museum
recommender service might suggest a colourful medieval painting to a
user who seems to like colourful paintings by the modern artist Keith
Haring.  User behaviour (e.g.~following up on these recommendations)
is outside of the direct control of the system and may serve as a
\textbf{serendipity trigger}, and change the way it makes
recommendations in the future.  The system has a \textbf{prepared
  mind}, including both a \emph{user model} and a \emph{domain model},
both of which can be updated dynamically.  The connections through
which recommendations are made usually happen when the system notices
that elements of the domain have something in common via clustering or
faceting.  A \textbf{bridge} to a new kind of recommendation may be
found if new elements are introduced into the domain which do not
cluster well, or if the user appears to know about different clusters
that do not have obvious connections between them.  The intended
outcome of recommendations depend on the organisational mission
e.g.~to make money, to provide a good user experience, etc.; at the
system level, the serendiptious \textbf{result} would be learning a
new approach that helps to address these goals better.

From the perspective of our model, \textbf{chance} will only have a
significant role when the system has the capacity to learn from user
behaviour.  In fact, Bayesian methods are used in contemporary
recommender systems (surveyed in Chapter 3 of
\citeNP{shengbo-guo-thesis}).  The typical commercial perspective on
recommendations is related to the process of ``conversion'' -- turning
recommendations into clicks and clicks into purchases.  Combined with
the ability to learn, \textbf{curiosity} could be described as the
urge to make ``outside-the-box''\footnote{\citeA{abbassi2009getting}.}
recommendations specifically for the purposes of learning more about
users, possibly to the detriment of other goals over the short term.
Measures of \textbf{sagacity} would relate to the system's ability to
draw inferences from user behaviour that would update the
recommendation model.  For example, the system might do A/B testing to
decide how novel recommendation strategies influence conversion.  The
\textbf{value} of recommendation strategies can be measured in terms
of traditional business metrics or other organisational objectives.

A \textbf{dynamic world} which nevertheless exhibits some regularity
is a precondition for useful A/B testing.  As mentioned above the
primary \textbf{(multiple) contexts} are the user model and the domain
model.  A system matching the description here would have
\textbf{multiple tasks}: making useful recommendations, generating new
experiments to learn about users, and building new models based on the
results of these experiments.  Such a system could avail itself of
\textbf{multiple influences} related to experimental design,
psychology, and domain understanding.

% As a general comment, we would say that this is largely how
% \emph{research and development} of recommender systems works, but
% without the same levels of system automony envisioned here.
