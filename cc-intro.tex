\section{Serendipity in a computational context} \label{sec:computational-serendipity}

The 13 criteria from Section \ref{sec:literature-review} specify the
conditions and preconditions that are conducive to serendipitous
discovery.  Here, we revisit each of these criteria and briefly
summarise how they can be thought about from a computational point of
view, again focusing on examples.  We then present a thought
experiment that evaluates the ideas described above in the course of
developing a new system design.

% \input{writers-workshop-background-long}

\subsection{Prior partial examples}

\textbf{[Jazz, recommender systems, HR.]}

Research in recommender systems focusses on stimulating serendipity on the user side, by suggesting items that might more likely be unexpected, potentially novel, and necessarily of value to the user. Several components of the proposed model were implemented in related work, and art and music recommendation connects to the area of computational creativity.

A recommender system must come with a prepared mind in terms of full knowledge of the items in the search space, and of the user's partial knowledge of their existence and properties. Predictions are based on existing knowledge of the items known to the user, and his or her preferences. The system's goal is to recommend an item as result which is unexpected, valuable and potentially novel to a specific user, while the user's goal is to find items that are valuable in different respects. 

Related work tries to structure the search space and exploit patterns as serendipity triggers. For example, \cite{Herlocker2004} as well as \cite{Lu2012} associate less popular items with a higher unexpectedness. Clustering was also frequently used to discover latent structures in the search space. For example, \cite{Kamahara2005} partition users into clusters of common interest, while \cite{Onuma2009} as well as \cite{Zhang2011} perform clustering on both users and items. In the work by \cite{Oku2011}, the user is allowed to select two items in order to mix their features in some sort of conceptual blending. 

If a pattern is found, it is used to bridge between items that are known and valuable to the user, and those that are potentially unexpected. As an example,  \cite{Sugiyama2011} connects users with divergent interests, while \cite{Onuma2009} weight items stronger that bridge between topical clusters. 

Recommender systems have to cope with a dynamic world of user preferences and new items that are introduced to the system. The imperfect knowledge about the user's preferences and interests represents perhaps the strongest dimension of chance. Determining the value of an item, both in terms of value for the user and unexpectedness, is of paramount importance. While standardized measures such as the $F_1$-score or the (R)MSE )are used to determine the  accuracy of an evaluation in terms of preferred items in the user's history, there is no common agreement on a measure for serendipity yet \cite{ Murakami2008, Adamopoulos2011, McCay-Peet2011}.