\section{Serendipity in computational systems} \label{sec:computational-serendipity}

The 13 criteria from Section \ref{sec:literature-review} specify the
conditions and preconditions that are conducive to serendipitous
discovery.  These criteria have been further formalised
in Section \ref{specs-overview}.
% 
\citeA{pease2013discussion} used a slightly different version of the
SPECS criteria to discuss three examples of serendipitous behaviour:
in dynamic investigation problems, model generation, and poetry
flowcharts.  Two additional examples using the revised criteria are
described below.  These example serve the purpose of illustrating our
revised criteria, and also show forays of computational intelligence
into domains known for serendipity in their everyday cultural context.
We then turn to a more elaborated thought experiment that evaluates
these ideas in the course of developing a new system design.

% \input{writers-workshop-background-long}

\subsection{Case Studies: Prior art}

\paragraph{Evolutionary music improvisation systems.}

\citeA{jordanous10} reported a computational jazz improvisation system using genetic algorithms. Genetic algorithms, and evolutionary computing more generally, could encourage computational serendipity. We examine Jordanous's system (later given the name {\em GAmprovising} \cite{jordanous:12}) as a case study for evolutionary computing in the context of our model of computational serendipity: to what extent does GAmprovising model serendipity?

GAmprovising uses genetic algorithms to evolve a population of Improvisors. Each Improvisor is able to randomly generate music based on various parameters such as range of notes to be used, preferred notes to be used, rhythmic implications around note lengths and other musical parameters (see \cite{jordanous10}. These parameters are what defines the Improvisor at any point in evolution. After a cycle of evolution, each Improvisor is evaluated via a fitness function based on Ritchie's criteria \cite{ritchie07} of how creative the Improvisor is. Ritchie's criteria use user-supplied ratings of how novel and how appropriate the music produced by the Improvisor is, to calculate 18 criteria that collectively evaluate how creative a system is. The most successful Improvisors (as deemed by the fitness function) are used to seed a new generation of Improvisors, through crossover and mutation operations.

The GAmprovising system can be said to have a \textbf{prepared mind} through its background knowledge of what musical knowledge to embed in the Improvisors and the evolutionary abilities to evolve Improvisors. A \textbf{serendipity trigger} comes from the combination of the mutation and crossover operations employed in the genetic algorithm, and the user input feeding into the fitness function to evaluate produced music. A \textbf{bridge}, from the genetic algorithm operations and user input, to the result is built by the creation of new Improvisors. The \textbf{results} are the various musical improvisations produced by the fittest Improvisors (as well as, perhaps, the parameters that have been considered fittest).

The likelihood of serendipitous evolution is greatly enhanced by the use of mutation and crossover operations within the genetic algorithm, to increase the diversity of search space covered by the system during evolution. However the \textbf{chance} of any particular Improvisor being discovered is low, given the massive dimensions of the search space.  Interesting developments in evolution would be guided by \textbf{curiosity} through the particular human user identifying Improvisors as interesting at that time. \textbf{Sagacity} is determined by how likely the user is to appreciate the same Improvisor's music (or similar music) over time, as tastes of the user may change. The \textbf{value} of the results are maximised through employing a fitness function.

Evolutionary systems such as GAmprovising necessarily operate in a \textbf{dynamic world} which is evolving continuously and may also be affected by changes in user tastes as they evaluate musical output from Improvisors. The \textbf{multiple contexts} arise from the possibility of having multiple users evaluate the musical output (though this is as yet not implemented formally) or through the user changing their preferences over time. \textbf{Multiple tasks} are carried out by the system including evolution of Improvisors, generation of music by individual Improvisors, capturing of user ratings of a sample of the Improvisors' output, and fitness calculations. \textbf{Multiple influences} are captured through the various combinations of parameters that could be set and the potential range of values for each parameter.

% \paragraph{{[}To add: HR.{]}}

\paragraph{Recommender systems.} 

As discussed in Section \ref{sec:related}, recommender systems are one
of the primary contexts in computing where serendipity is seen to play
a role.  As we noted, these systems mostly focus on discovery.
Nevertheless, certain architectures that also take account of
invention would match all of criteria described by our model.  Here we
draw on the observation that recommender systems not \emph{stimulate}
serendipitous discovery for the user: they also have the task of
\emph{simulating} when this is likely to occur.

A recommendation is typically provided if the system suspects that the
item will be likely to introduce ideas that are close to what the user
knows, but that will be unexpected.  In other words, the system aims
to stimulate serendipity for the user. For example, a museum
recommender service might suggest a colourful medieval painting to a
user who seems to like colourful paintings by the modern artist Keith
Haring.  User behaviour (e.g.~following up on these recommendations)
is outside of the direct control of the system and may serve as a
\textbf{serendipity trigger}, and change the way it makes
recommendations in the future.  The system has a \textbf{prepared
  mind}, including both a \emph{user model} and a \emph{domain model},
both of which can be updated dynamically.  The connections through
which recommendations are made usually happen when the system notices
that elements of the domain have something in common via clustering or
faceting.  A \textbf{bridge} to a new kind of recommendation may be
found if new elements are introduced into the domain which do not
cluster well, or if the user appears to know about different clusters
that do not have obvious connections between them.  The intended
outcome of recommendations depend on the organisational mission
e.g.~to make money, to provide a good user experience, etc.; at the
system level, the serendiptious \textbf{result} would be learning a
new approach that helps to address these goals better.

From the perspective of our model, \textbf{chance} will only have a
significant role when the system has the capacity to learn from user
behaviour.  In fact, Bayesian methods are used in contemporary
recommender systems (surveyed in Chapter 3 of
\citeNP{shengbo-guo-thesis}).  The typical commercial perspective on
recommendations is related to the process of ``conversion'' -- turning
recommendations into clicks and clicks into purchases.  Combined with
the ability to learn, \textbf{curiosity} could be described as the
urge to make ``outside-the-box''\footnote{\citeA{abbassi2009getting}.}
recommendations specifically for the purposes of learning more about
users, possibly to the detriment of other goals over the short term.
Measures of \textbf{sagacity} would relate to the system's ability to
draw inferences from user behaviour that would update the
recommendation model.  For example, the system might do A/B testing to
decide how novel recommendation strategies influence conversion.  The
\textbf{value} of recommendation strategies can be measured in terms
of traditional business metrics or other organisational objectives.

A \textbf{dynamic world} which nevertheless exhibits some regularity
is a precondition for useful A/B testing.  As mentioned above the
primary \textbf{(multiple) contexts} are the user model and the domain
model.  A system matching the description here would have
\textbf{multiple tasks}: making useful recommendations, generating new
experiments to learn about users, and building new models based on the
results of these experiments.  Such a system could avail itself of
\textbf{multiple influences} related to experimental design,
psychology, and domain understanding.

% As a general comment, we would say that this is largely how
% \emph{research and development} of recommender systems works, but
% without the same levels of system automony envisioned here.
\small
\begin{table}[ht]%dp]
\caption{Summary of case studies as viewed with our computational serendipity model}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
{\textbf Model part}  & Evolutionary music systems & Recommender systems \\
\hline
\hline
%{\em Key Condition}  && \\
%Focus shift && \\
%\hline
{\em Components} && \\
\hline
\hline
Serendipity trigger && Input from user behaviour \\
\hline
Prepared mind  & & Through user model/domain model \\
\hline
Bridge  & & Elements identified outside clusters \\
\hline
Result & & Dependent on organisation goals \\
\hline
\hline
{\em Dimensions} && \\
\hline
\hline
Chance & & If learning from user behaviour \\
\hline
Curiosity & & Making unusual recommendations \\
\hline
Sagacity & & Updating models after user behaviour \\
\hline
Value & & As per business metrics/objectives \\
\hline
\hline
{\em Environmental} && \\
{\em Factors} && \\
\hline
\hline
Dynamic && As precondition for testing system's \\
 world && \hspace{3mm} influences on user behaviour\\
\hline
Multiple && User model and domain model\\
contexts && \\
\hline
Multiple && Making recommendations, learning\\
 tasks && \hspace{3mm}from users, updating models \\
\hline
Multiple && Experimental design, psychology, \\
influences && \hspace{3mm} domain understanding\\
\hline
\end{tabular}
\end{center}
\label{caseStudies}
\end{table}%
\normalsize
